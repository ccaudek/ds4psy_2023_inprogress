
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>La verosimiglianza &#8212; ds4psy</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Credibilità, modelli e parametri" href="025_intro_bayes.html" />
    <link rel="prev" title="Distribuzioni di v.c. continue" href="023_cont_rv_distr.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">ds4psy</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Nozioni di base
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Prefazione
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001_key_notions.html">
   Concetti chiave
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="005_measurement.html">
   La misurazione in psicologia
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007_freq_distr.html">
   Analisi esplorativa dei dati
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="011_loc_scale.html">
   Indici di posizione e di scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="012_correlation.html">
   Le relazioni tra variabili
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="013_eda_quickstart.html">
   Introduzione alla data analisi
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probabilità
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="014_dice.html">
   Calcolo combinatorio
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="015_intro_prob.html">
   La logica dell’incerto
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="016_conditional_prob.html">
   Probabilità condizionata: significato, teoremi, eventi indipendenti
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="017_bayes_theorem.html">
   Il teorema di Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="018_expval_var.html">
   Indici di posizione, di varianza e di associazione di variabili casuali
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="019_joint_prob.html">
   Probabilità congiunta
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="020_density_func.html">
   La densità di probabilità
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Distribuzioni di v.c.
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="022_discr_rv_distr.html">
   Distribuzioni di v.c. discrete
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="023_cont_rv_distr.html">
   Distribuzioni di v.c. continue
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   La verosimiglianza
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Inferenza bayesiana
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="025_intro_bayes.html">
   Credibilità, modelli e parametri
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="026_subj_prop.html">
   Pensare ad una proporzione in termini soggettivi
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="029_conjugate_families.html">
   Distribuzioni coniugate
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="030_balance_prior_post.html">
   L’influenza della distribuzione a priori
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="036_metropolis.html">
   Approssimazione della distribuzione a posteriori
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="040_beta_binomial.html">
   Markov Chain Monte Carlo per l’inferenza bayesiana
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="050_normal_normal_mod.html">
   Inferenza su una media
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Regressione lineare
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="051_reglin_1.html">
   Introduzione
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="052_reglin_2.html">
   Regressione lineare bivariata
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="053_reglin_3.html">
   Regressione lineare con PyMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="054_reglin_4.html">
   Confronto tra le medie di due gruppi
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Inferenza frequentista
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="220_intro_frequentist.html">
   Legge dei grandi numeri
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221_conf_interv.html">
   Intervallo fiduciale
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="226_test_ipotesi.html">
   Significatività statistica
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="228_limiti_stat_frequentista.html">
   Limiti dell’inferenza frequentista
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="230_s_m_errors.html">
   Errori di tipo
   <em>
    m
   </em>
   (magnitude) e di tipo
   <em>
    s
   </em>
   (sign)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bibliografia
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="z_biblio.html">
   Bibliografia
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendici
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="a01_math_symbols.html">
   Simbologia di base
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="a02_number_sets.html">
   Numeri binari, interi, razionali, irrazionali e reali
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="a04_summation_notation.html">
   Simbolo di somma (sommatorie)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="a05_calculus_notation.html">
   Per liberarvi dai terrori preliminari
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/ccaudek/ds4psy_2023/master?urlpath=tree/docs/024_likelihood.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ccaudek/ds4psy_2023"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ccaudek/ds4psy_2023/issues/new?title=Issue%20on%20page%20%2F024_likelihood.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/024_likelihood.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#la-funzione-di-verosimiglianza">
   La funzione di verosimiglianza
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modello-binomiale">
   Modello binomiale
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretazione">
     Interpretazione
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-log-verosimiglianza">
     La log-verosimiglianza
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modello-gaussiano">
   Modello gaussiano
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#una-singola-osservazione">
     Una singola osservazione
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#un-campione-di-osservazioni">
     Un campione di osservazioni
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#massima-verosimiglianza">
     Massima verosimiglianza
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">
   Commenti e considerazioni finali
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>La verosimiglianza</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#la-funzione-di-verosimiglianza">
   La funzione di verosimiglianza
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modello-binomiale">
   Modello binomiale
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretazione">
     Interpretazione
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-log-verosimiglianza">
     La log-verosimiglianza
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modello-gaussiano">
   Modello gaussiano
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#una-singola-osservazione">
     Una singola osservazione
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#un-campione-di-osservazioni">
     Un campione di osservazioni
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#massima-verosimiglianza">
     Massima verosimiglianza
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">
   Commenti e considerazioni finali
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="la-verosimiglianza">
<span id="cap-likelihood"></span><h1>La verosimiglianza<a class="headerlink" href="#la-verosimiglianza" title="Permalink to this headline">#</a></h1>
<p>La verosimiglianza viene utilizzata sia nell’inferenza bayesiana che in quella frequentista. In entrambi i paradigmi di inferenza, il suo ruolo è quello di quantificare la forza con la quale i dati osservati supportano i possibili valori dei parametri sconosciuti di un modello statistico.</p>
<section id="la-funzione-di-verosimiglianza">
<h2>La funzione di verosimiglianza<a class="headerlink" href="#la-funzione-di-verosimiglianza" title="Permalink to this headline">#</a></h2>
<p>La <em>funzione di verosimiglianza</em> <span class="math notranslate nohighlight">\(\mathcal{L}(\theta \mid y) = f(y \mid \theta), \theta \in \Theta,\)</span> è la funzione di massa o di densità di probabilità dei dati <span class="math notranslate nohighlight">\(y\)</span> vista come una funzione del parametro sconosciuto (o dei parametri sconosciuti) <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Detto in altre parole, la funzione di verosimiglianza e la funzione di (massa o densità di) probabilità sono formalmente identiche, ma è completamente diversa la loro interpretazione:</p>
<ul class="simple">
<li><p>nel caso della funzione di massa o di densità di probabilità, la distribuzione del vettore casuale delle osservazioni campionarie <span class="math notranslate nohighlight">\(y\)</span> dipende dai valori assunti dal parametro (o dai parametri) <span class="math notranslate nohighlight">\(\theta\)</span> – per esempio, nel caso della distribuzione binomiale, fissata <span class="math notranslate nohighlight">\(\theta\)</span> (probabilità di successo) a 0.5, la probabilità di osservare <span class="math notranslate nohighlight">\(y = 0, \dots, 10\)</span> successi in <span class="math notranslate nohighlight">\(n\)</span> prove è determinata in maniera univoca (se il valore del parametro <span class="math notranslate nohighlight">\(\theta\)</span> è noto, quello che resta da stabilire è la probabilità da assegnare a ciascuno degli esiti <span class="math notranslate nohighlight">\(y\)</span> possibili);</p></li>
<li><p>nel caso della la funzione di verosimiglianza la credibilità assegnata a ciascun possibile valore <span class="math notranslate nohighlight">\(\theta\)</span> viene determinata avendo acquisita l’informazione campionaria <span class="math notranslate nohighlight">\(y\)</span> che rappresenta l’elemento condizionante (in questo secondo caso, <span class="math notranslate nohighlight">\(y\)</span> è noto, ma <span class="math notranslate nohighlight">\(\theta\)</span> è ignoto; ci chiediamo quale sia la credibilità relativa di ciascuno dei possibili valori <span class="math notranslate nohighlight">\(\theta\)</span>, avendo osservato un determinato <span class="math notranslate nohighlight">\(y\)</span>).</p></li>
</ul>
<p>La funzione di verosimiglianza descrive dunque in termini relativi il sostegno empirico che <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> riceve da <span class="math notranslate nohighlight">\(y\)</span>. Infatti, la funzione di verosimiglianza assume forme diverse al variare di <span class="math notranslate nohighlight">\(y\)</span>. Possiamo dunque pensare alla funzione di verosimiglianza come alla risposta alla seguente domanda: avendo osservato i dati <span class="math notranslate nohighlight">\(y\)</span>, quanto risultano (relativamente) credibili i diversi valori del parametro <span class="math notranslate nohighlight">\(\theta\)</span>? In termini più formali possiamo dire che, sulla base dei dati, <span class="math notranslate nohighlight">\(\theta_1 \in \Theta\)</span> risulta più credibile di <span class="math notranslate nohighlight">\(\theta_2 \in \Theta\)</span> quale indice del modello probabilistico generatore dei dati se <span class="math notranslate nohighlight">\(\mathcal{L}(\theta_1) &gt; \mathcal{L}(\theta_1)\)</span>.</p>
<p>Si noti un punto importante: la funzione <span class="math notranslate nohighlight">\(\mathcal{L}(\theta \mid y)\)</span> non è una funzione di densità. Infatti, essa non racchiude un’area unitaria.</p>
</section>
<section id="modello-binomiale">
<h2>Modello binomiale<a class="headerlink" href="#modello-binomiale" title="Permalink to this headline">#</a></h2>
<p>Per chiarire il concetto di verosimiglianza consideriamo innanzitutto il caso più semplice, ovvero quello Binomiale.</p>
<p>Per <span class="math notranslate nohighlight">\(n\)</span> prove Bernoulliane indipendenti, le quali producono <span class="math notranslate nohighlight">\(y\)</span> successi e (<span class="math notranslate nohighlight">\(n-y\)</span>) insuccessi, la funzione nucleo di verosimiglianza (ovvero, la funzione di verosimiglianza da cui sono state escluse tutte le costanti moltiplicative che non hanno alcun effetto su <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>) è</p>
<div class="math notranslate nohighlight" id="equation-eq-like-binomial-kernel">
<span class="eqno">(55)<a class="headerlink" href="#equation-eq-like-binomial-kernel" title="Permalink to this equation">#</a></span>\[
\mathcal{L}(p \mid y) = \theta^y (1-\theta)^{n - y}.\notag
\]</div>
<p>Per fare un esempio pratico, consideriamo la ricerca di <span id="id1">[<a class="reference internal" href="z_biblio.html#id10" title="Ulrike Zetsche, Paul-Christian Buerkner, and Babette Renneberg. Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7):678, 2019.">ZBR19</a>]</span>. Questi ricercatori hanno trovato che, su 30 pazienti clinicamente depressi, 23 manifestavano delle aspettative distorsione negativamente relativamente al loro umore futuro. Se i dati di <span id="id2">[<a class="reference internal" href="z_biblio.html#id10" title="Ulrike Zetsche, Paul-Christian Buerkner, and Babette Renneberg. Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7):678, 2019.">ZBR19</a>]</span> vengono riassunti mediante una proporzione (ovvero, 23/30), allora è sensato adottare un modello probabilistico binomiale quale meccanismo generatore dei dati:</p>
<div class="math notranslate nohighlight" id="equation-eq-binomialmodel">
<span class="eqno">(56)<a class="headerlink" href="#equation-eq-binomialmodel" title="Permalink to this equation">#</a></span>\[
y  \sim Bin(n, \theta),
\]</div>
<p>laddove <span class="math notranslate nohighlight">\(\theta\)</span> è la probabiltà che una prova Bernoulliana assuma il valore 1 e <span class="math notranslate nohighlight">\(n\)</span> corrisponde al numero di prove Bernoulliane. Questo modello assume che le prove Bernoulliane <span class="math notranslate nohighlight">\(y\)</span> che costituiscono il campione siano tra loro indipendenti e che ciascuna abbia la stessa probabilità <span class="math notranslate nohighlight">\(\theta \in [0, 1]\)</span> di essere un “successo” (valore 1). In altre parole, il modello generatore dei dati avrà la seguente funzione di massa di probabilità</p>
<div class="math notranslate nohighlight">
\[
p(y \mid \theta)
\ = \
Bin(y \mid n, \theta).
\]</div>
<p>Nei capitoli precedenti è stato mostrato come, sulla base del modello binomiale, sia possibile assegnare una probabilità a ciascun possibile valore <span class="math notranslate nohighlight">\(y \in \{0, 1, \dots, n\}\)</span> <em>assumendo noto il valore del parametro</em> <span class="math notranslate nohighlight">\(\theta\)</span>. Ma ora abbiamo il problema inverso, ovvero quello di fare inferenza su <span class="math notranslate nohighlight">\(\theta\)</span> alla luce dei dati campionari <span class="math notranslate nohighlight">\(y\)</span>. In altre parole, riteniamo di conoscere il modello probabilistico che ha generato i dati, ma di tale modello non conosciamo i parametri: vogliamo dunque ottenere informazioni su <span class="math notranslate nohighlight">\(\theta\)</span> avendo osservato i dati <span class="math notranslate nohighlight">\(y\)</span>. Per fare questo, in un ottica bayesiana, è innanzitutto necessario definire la funzione di verosimiglianza.</p>
<p>Per i dati di <span id="id3">[<a class="reference internal" href="z_biblio.html#id10" title="Ulrike Zetsche, Paul-Christian Buerkner, and Babette Renneberg. Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7):678, 2019.">ZBR19</a>]</span>, la funzione di verosimiglianza corrisponde alla funzione binomiale di parametro <span class="math notranslate nohighlight">\(\theta \in [0, 1]\)</span> sconosciuto. Abbiamo osservato <span class="math notranslate nohighlight">\(y\)</span> = 23 successi, in <span class="math notranslate nohighlight">\(n\)</span> = 30 prove.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">23</span>
</pre></div>
</div>
</div>
</div>
<p>La funzione di verosimiglianza dunque diventa</p>
<div class="math notranslate nohighlight" id="equation-eq-likebino23">
<span class="eqno">(57)<a class="headerlink" href="#equation-eq-likebino23" title="Permalink to this equation">#</a></span>\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} \theta^{23} + (1-\theta)^7.
\]</div>
<p>Per costruire la funzione di verosimiglianza dobbiamo applicare l’eq. <a class="reference internal" href="#equation-eq-likebino23">(57)</a> tante volte, cambiando ogni volta il valore <span class="math notranslate nohighlight">\(\theta\)</span>, ma tenendo sempre costante il valore dei dati. Nella seguente simulazione considereremo 100 possibili valori <span class="math notranslate nohighlight">\(\theta \in [0, 1]\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>

<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505
 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111
 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717
 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323
 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929
 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535
 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141
 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747
 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354
 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596
 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566
 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172
 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778
 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384
 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899
 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596
 0.96969697 0.97979798 0.98989899 1.        ]
</pre></div>
</div>
</div>
</div>
<p>Per esempio, ponendo <span class="math notranslate nohighlight">\(\theta = 0.1\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9.7371682902e-18
</pre></div>
</div>
</div>
</div>
<p>Ponendo <span class="math notranslate nohighlight">\(\theta = 0.2\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.58141723492221e-11
</pre></div>
</div>
</div>
</div>
<p>Se ripetiamo questo processo 100 volte, una volta per ciascuno dei valori <span class="math notranslate nohighlight">\(\theta\)</span> considerati, otteniamo 100 coppie di punti <span class="math notranslate nohighlight">\(\theta\)</span> e <span class="math notranslate nohighlight">\(f(\theta)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">like</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>La curva che interpola tali punti è la funzione di verosimiglianza. La figura successiva fornisce una rappresentazione grafica di tale funzione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">23</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">like</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Funzione di verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Valore della variabile casuale theta [0, 1]&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/024_likelihood_11_0.png" src="_images/024_likelihood_11_0.png" />
</div>
</div>
<section id="interpretazione">
<h3>Interpretazione<a class="headerlink" href="#interpretazione" title="Permalink to this headline">#</a></h3>
<p>Come possiamo interpretare la curva che abbiamo ottenuto? Per alcuni valori <span class="math notranslate nohighlight">\(\theta\)</span> la funzione di verosimiglianza assume valori piccoli; per altri valori <span class="math notranslate nohighlight">\(\theta\)</span> la funzione di verosimiglianza assume valori più grandi. Questi ultimi sono i valori <span class="math notranslate nohighlight">\(\theta\)</span> più credibili e il valore 23/30 = 0.767 (la moda della funzione di verosimiglianza) è il valore più credibile di tutti.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_list</span> <span class="o">=</span> <span class="n">like</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>
<span class="nb">max</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">input_list</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">input_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="nb">max</span><span class="p">:</span>
        <span class="nb">max</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">i</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Index of the maximum value is : </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index of the maximum value is : 76
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">[</span><span class="mi">76</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7676767676767677
</pre></div>
</div>
</div>
</div>
<p>Si noti che, anziché usare la funzione <code class="docutils literal notranslate"><span class="pre">like()</span></code> che (per chiarezza) abbiamo definito sopra, in una maniera del tutto equivalente è possibile usare la funzione <code class="docutils literal notranslate"><span class="pre">binom.pmf()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">23</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span>  <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Funzione di verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Valore della variabile casuale theta [0, 1]&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/024_likelihood_16_0.png" src="_images/024_likelihood_16_0.png" />
</div>
</div>
</section>
<section id="la-log-verosimiglianza">
<h3>La log-verosimiglianza<a class="headerlink" href="#la-log-verosimiglianza" title="Permalink to this headline">#</a></h3>
<p>Dal punto di vista pratico risulta più conveniente utilizzare, al posto della funzione di verosimiglianza, il suo logaritmo naturale, ovvero la funzione di log-verosimiglianza:</p>
<div class="math notranslate nohighlight" id="equation-eq-loglike-definition">
<span class="eqno">(58)<a class="headerlink" href="#equation-eq-loglike-definition" title="Permalink to this equation">#</a></span>\[
\ell(\theta) = \log \mathcal{L}(\theta).
\]</div>
<p>Poiché il logaritmo è una funzione strettamente crescente (usualmente si considera il logaritmo naturale), allora <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> e <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> assumono il massimo (o i punti di massimo) in corrispondenza degli stessi valori di <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta} = argmax_{\theta \in \Theta} \ell(\theta) = argmax_{\theta \in \Theta} \mathcal{L}(\theta).
\]</div>
<p>Per le proprietà del logaritmo, la funzione nucleo di log-verosimiglianza della binomiale è</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell(\theta \mid y) &amp;= \log \mathcal{L}(\theta \mid y) \notag\\
          &amp;= \log \left(\theta^y (1-\theta)^{n - y} \right) \notag\\
          &amp;= \log \theta^y + \log \left( (1-\theta)^{n - y} \right) \notag\\
          &amp;= y \log \theta + (n - y) \log (1-\theta).\notag
\end{aligned}
\end{split}\]</div>
<p>Si noti che non è necessario lavorare con i logaritmi, ma è fortemente consigliato. Il motivo è che i valori della verosimiglianza, in cui si moltiplicano valori di probabilità molto piccoli, possono diventare estremamente piccoli – qualcosa come <span class="math notranslate nohighlight">\(10^{-34}\)</span>. In tali circostanze, non è sorprendente che i programmi dei computer mostrino problemi di arrotondamento numerico. Le trasformazioni logaritmiche risolvono questo problema.</p>
<p>Svolgiamo nuovamente il problema precedente usando la log-verosimiglianza per trovare il massimo della funzione di log-verosimiglianza. Ora utilizziamo la funzione <code class="docutils literal notranslate"><span class="pre">binom.logpmf()</span></code>.</p>
<p>La funzione di log-verosimiglianza è rappresentata nella figura successiva.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">23</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Funzione di log-verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Valore della variabile casuale theta [0, 1]&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log-verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/024_likelihood_18_0.png" src="_images/024_likelihood_18_0.png" />
</div>
</div>
<p>Il risultato replica quello trovato in precedenza con la funzione di verosimiglianza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_list</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">max</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">input_list</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">input_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="nb">max</span><span class="p">:</span>
        <span class="nb">max</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">i</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Index of the maximum value is : </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index of the maximum value is : 76
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7676767676767677
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="modello-gaussiano">
<h2>Modello gaussiano<a class="headerlink" href="#modello-gaussiano" title="Permalink to this headline">#</a></h2>
<p>Ora che abbiamo capito come costruire la funzione verosimiglianza di una binomiale è relativamente semplice fare un passo ulteriore e considerare la verosimiglianza del caso di una funzione di densità, ovvero nel caso di una variabile casuale continua. Consideriamo qui il caso della Normale. La densità di una distribuzione Normale di parametri <span class="math notranslate nohighlight">\(\mu\)</span> e <span class="math notranslate nohighlight">\(\sigma\)</span> è</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-sim-like">
<span class="eqno">(59)<a class="headerlink" href="#equation-eq-gaussian-sim-like" title="Permalink to this equation">#</a></span>\[
f(y \mid \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left\{-\frac{1}{2\sigma^2}(y-\mu)^2\right\}.
\]</div>
<p>Costruiamo dunque la funzione di verosimiglianza nel caso dell’equazione <a class="reference internal" href="#equation-eq-gaussian-sim-like">(59)</a>.</p>
<section id="una-singola-osservazione">
<h3>Una singola osservazione<a class="headerlink" href="#una-singola-osservazione" title="Permalink to this headline">#</a></h3>
<p>Esaminiamo prima il caso in cui i dati corrispondono ad una singola osservazione <span class="math notranslate nohighlight">\(y\)</span>. Poniamo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mi">114</span>
</pre></div>
</div>
</div>
</div>
<p>L’equazione <a class="reference internal" href="#equation-eq-gaussian-sim-like">(59)</a> dipende dai parametri <span class="math notranslate nohighlight">\(\mu\)</span> e <span class="math notranslate nohighlight">\(\sigma\)</span> e dai dati <span class="math notranslate nohighlight">\(y\)</span>. Per semplicità, ipotizziamo <span class="math notranslate nohighlight">\(\sigma\)</span> noto e uguale a 15. Nell’esercizio considereremo 1000 valori <span class="math notranslate nohighlight">\(\mu\)</span> compresi tra 70 e 160.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">70.0</span><span class="p">,</span> <span class="mf">160.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Dato che consideriamo 1000 possibili valori <span class="math notranslate nohighlight">\(\mu\)</span>, per costruire la funzione di verosimiglianza applicheremo 1000 volte l’eq. <a class="reference internal" href="#equation-eq-gaussian-sim-like">(59)</a>. In ciascun passo dell’esercizio inseriremo nell’&#64;eq-gaussian-sim-like</p>
<ul class="simple">
<li><p>il singolo valore <span class="math notranslate nohighlight">\(y\)</span> considerato (che viene mantenuto costante),</p></li>
<li><p>il valore <span class="math notranslate nohighlight">\(\sigma\)</span> assunto noto (anch’esso costante),</p></li>
<li><p>uno alla volta ciascuno dei valori <span class="math notranslate nohighlight">\(\mu\)</span> che abbiamo definito sopra (quindi, nelle 1000 applicazioni dell’eq. <a class="reference internal" href="#equation-eq-gaussian-sim-like">(59)</a>, il valore <span class="math notranslate nohighlight">\(\mu\)</span> è l’unico che varia: <span class="math notranslate nohighlight">\(y\)</span> e <span class="math notranslate nohighlight">\(\sigma\)</span> sono mantenuti costanti).</p></li>
</ul>
<p>La distribuzione Gaussiana è implementata in Python mediante <code class="docutils literal notranslate"><span class="pre">norm.pdf()</span></code>. La funzione <code class="docutils literal notranslate"><span class="pre">norm.pdf()</span></code> richiede tre argomenti: il valore <span class="math notranslate nohighlight">\(y\)</span> (o il vettore <span class="math notranslate nohighlight">\(y\)</span>), la media, ovvero il parametro <span class="math notranslate nohighlight">\(\mu\)</span>, e la deviazione standard, ovvero il parametro <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>Applicando la funzione <code class="docutils literal notranslate"><span class="pre">norm.pdf()</span></code> 1000 volte, una volta per ciascuno dei valori <span class="math notranslate nohighlight">\(\mu\)</span> che abbiamo definito (e tenendo fissi <span class="math notranslate nohighlight">\(y = 114\)</span> e <span class="math notranslate nohighlight">\(\sigma = 15\)</span>), otteniamo 1000 valori <span class="math notranslate nohighlight">\(f(\mu)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">f_mu</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>La funzione di verosimiglianza è la curva che interpola i punti <span class="math notranslate nohighlight">\(\big(\mu, f(\mu)\big)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">f_mu</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Funzione di verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Valore della variabile casuale mu [70, 160]&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">70</span><span class="p">,</span> <span class="mi">160</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/024_likelihood_30_0.png" src="_images/024_likelihood_30_0.png" />
</div>
</div>
<p>La funzione di verosimiglianza così trovata ha la forma della distribuzione Gaussiana. Nel caso di una singola osservazione, <em>ma solo in questo caso</em>, ha anche un’area unitaria. Per l’esempio presente, la moda della funzione di verosimiglianza è 114.</p>
</section>
<section id="un-campione-di-osservazioni">
<h3>Un campione di osservazioni<a class="headerlink" href="#un-campione-di-osservazioni" title="Permalink to this headline">#</a></h3>
<p>Consideriamo ora il caso più generale, ovvero quello di un campione di <span class="math notranslate nohighlight">\(n\)</span> osservazioni. Possiamo immaginare un campione casuale <span class="math notranslate nohighlight">\(y_1, y_2, \dots, y_n\)</span> estratto da una popolazione <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma)\)</span> come una sequenza di realizzazioni indipendenti ed identicamente distribuite (di seguito, i.i.d.) della medesima variabile casuale <span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mu, \sigma)\)</span>. I parametri sconosciuti sono <span class="math notranslate nohighlight">\(\theta = \{\mu, \sigma\}\)</span>.</p>
<p>Se le variabili casuali <span class="math notranslate nohighlight">\(y_1, y_2, \dots, y_n\)</span> sono i.i.d., la loro densità congiunta è data da:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
f(y \mid \theta) &amp;= f(y_1 \mid \theta) \cdot f(y_2 \mid \theta) \cdot \; \dots \; \cdot f(y_n \mid \theta)\notag\\
                 &amp;= \prod_{i=1}^n f(y_i \mid \theta),
\end{align}
\end{split}\]</div>
<p>laddove <span class="math notranslate nohighlight">\(f(\cdot)\)</span> è la densità Gaussiana di parametri <span class="math notranslate nohighlight">\(\mu, \sigma\)</span>. Tenendo costanti i dati <span class="math notranslate nohighlight">\(y\)</span>, la funzione di verosimiglianza diventa:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\mathcal{L}(\theta \mid y) = \prod_{i=1}^n f(y_i \mid \theta).
\end{equation}
\]</div>
<p>Per chiarire la formula precedente, consideriamo un esempio che utilizza come dati i valori BDI-II dei trenta soggetti del campione clinico di <span id="id4">[<a class="reference internal" href="z_biblio.html#id10" title="Ulrike Zetsche, Paul-Christian Buerkner, and Babette Renneberg. Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7):678, 2019.">ZBR19</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">26</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> 
    <span class="mi">31</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">22</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Ci poniamo l’obiettivo di creare la funzione di verosimiglianza per questi dati, supponendo di sapere (in base ai risultati di ricerche precedenti) che i punteggi BDI-II si distribuiscono secondo la legge Normale e supponendo <span class="math notranslate nohighlight">\(\sigma\)</span> noto e uguale alla deviazione standard del campione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">true_sigma</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.495810615739622
</pre></div>
</div>
</div>
</div>
<p>Abbiamo visto in precedenza che, per una singola osservazione, la funzione di verosimiglianza è la densità Gaussiana espressa in funzione dei parametri (in questo caso, solo <span class="math notranslate nohighlight">\(\mu\)</span>). Per un campione di osservazioni i.i.d., ovvero <span class="math notranslate nohighlight">\(y = (y_1, y_2, \dots, y_n)\)</span>, la verosimiglianza è la funzione di densità congiunta <span class="math notranslate nohighlight">\(f(y \mid \mu, \sigma)\)</span> espressa in funzione dei parametri. Dato che le osservazioni sono i.i.d., la densità congiunta è data dal prodotto delle densità delle singole osservazioni.</p>
<p>Poniamoci il problema di trovare l’ordinata della funzione di log-verosimiglianza per le 30 osservazioni del campione in corrispondenza di <span class="math notranslate nohighlight">\(\mu = \mu_0\)</span></p>
<p>Per la prima osservazione del campione (<span class="math notranslate nohighlight">\(y_1 = 26\)</span>) abbiamo</p>
<div class="math notranslate nohighlight">
\[
f(26 \mid \mu_0, \sigma=6.50) = \frac{1}{{6.50 \sqrt {2\pi}}}\exp\left\{{-\frac{(26 - \mu_0)^2}{2\cdot 6.50^2}}\right\}.
\]</div>
<p>Se consideriamo tutte le osservazioni, la densità congiunta è data dal prodotto delle densità delle singole osservazioni: <span class="math notranslate nohighlight">\(f(y \mid \mu, \sigma = 6.50) = \, \prod_{i=1}^n f(y_i \mid \mu, \sigma = 6.50)\)</span>. Utilizzando i dati del campione, e assumendo <span class="math notranslate nohighlight">\(\sigma = 6.50\)</span>, l’ordinata della funzione di verosimiglianza in corrispondenza di <span class="math notranslate nohighlight">\(\mu_0\)</span> è uguale a</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}(\mu_0, \sigma=6.50 \mid y) =&amp; \, \prod_{i=1}^{30} f(y_i \mid \mu_0, \sigma = 6.50) = \notag\\
&amp; \frac{1}{{6.50 \sqrt {2\pi}}}\exp\left\{{-\frac{(26 - \mu_0)^2}{2\cdot 6.50^2}}\right\} \times \notag\\
 &amp; \frac{1}{{6.61 \sqrt {2\pi}}}\exp\left\{{-\frac{(35 - \mu_0)^2}{2\cdot 6.50^2}}\right\} \times  \notag\\
&amp; \vdots \notag\\
 &amp; \frac{1}{{6.61 \sqrt {2\pi}}}\exp\left\{{-\frac{(22 - \mu_0)^2}{2\cdot 6.50^2}}\right\}.
\end{aligned}
\end{split}\]</div>
<p>È più conveniente svolgere i calcoli usando il logaritmo della verosimiglianza. In Python definiamo la funzione di log-verosimiglianza, <code class="docutils literal notranslate"><span class="pre">log_likelihood()</span></code>, che prende come argomenti <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">mu</span></code> e <code class="docutils literal notranslate"><span class="pre">sigma</span> <span class="pre">=</span> <span class="pre">6.50</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">true_sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Consideriamo il valore <span class="math notranslate nohighlight">\(\mu_0 = \bar{y}\)</span>, ovvero</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bar_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bar_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>30.933333333333334
</pre></div>
</div>
</div>
</div>
<p>L’ordinata della funzione di log-verosimiglianza in corrispondenza di <span class="math notranslate nohighlight">\(\mu = 30.93\)</span> è</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">30.93</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">true_sigma</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-111.62269980698424
</pre></div>
</div>
</div>
</div>
<p>Troviamo ora i valori della log-verosimiglianza per ciascuno dei 1000 valori <span class="math notranslate nohighlight">\(\mu\)</span> nell’intervallo <span class="math notranslate nohighlight">\([\bar{y} - 2 \sigma, \bar{y} + 2 \sigma]\)</span>. Iniziamo a definire <code class="docutils literal notranslate"><span class="pre">mu</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Troviamo il valore dell’ordinata della funzione di log-verosimiglianza in corrispondenza di ciascuno dei 1000 valori <code class="docutils literal notranslate"><span class="pre">mu</span></code> che abbiamo definito.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ll</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu_val</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu_val</span> <span class="ow">in</span> <span class="n">mu</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Nel caso di un solo parametro sconosciuto (nel caso presente, <span class="math notranslate nohighlight">\(\mu\)</span>) è possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (<code class="docutils literal notranslate"><span class="pre">mu</span></code>, <code class="docutils literal notranslate"><span class="pre">ll</span></code>). Tale funzione descrive la <em>credibilità relativa</em> che può essere attribuita ai valori del parametro <span class="math notranslate nohighlight">\(\mu\)</span> alla luce dei dati osservati.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">ll</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Funzione di log-verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Valore della variabile casuale mu&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log-verosimiglianza&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/024_likelihood_46_0.png" src="_images/024_likelihood_46_0.png" />
</div>
</div>
</section>
<section id="massima-verosimiglianza">
<h3>Massima verosimiglianza<a class="headerlink" href="#massima-verosimiglianza" title="Permalink to this headline">#</a></h3>
<p>Il valore <span class="math notranslate nohighlight">\(\mu\)</span> più credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto <em>stima di massima verosimiglianza</em>.</p>
<p>Il massimo della funzione di log-verosimiglianza, ovvero 30.93 nel caso dell’esempio presente, è identico alla media dei dati campionari. Tale risultato, ottenuto per via numerica, può essere dimostrato formalmente (ma non lo faremo qui). Usando la notazione matematica possiamo dire che cerchiamo l’argmax dell’equazione precedente rispetto a <span class="math notranslate nohighlight">\(\theta\)</span>, ovvero</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta} = \text{argmax}_{\theta} \prod_{i=1}^n f(y_i \mid \theta).
\]</div>
<p>Questo problema si risolve calcolando le derivate della funzione rispetto a <span class="math notranslate nohighlight">\(\theta\)</span>, ponendo le derivate uguali a zero e risolvendo. Saltando tutti i passaggi algebrici di questo procedimento, per <span class="math notranslate nohighlight">\(\mu\)</span> si trova</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n y_i
\]</div>
<p>e</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma} = \sqrt{\sum_{i=1}^n\frac{1}{n}(y_i- \mu)^2}.
\]</div>
<p>In altri termini, la s.m.v. del parametro <span class="math notranslate nohighlight">\(\mu\)</span> è la media del campione e la s.m.v. del parametro <span class="math notranslate nohighlight">\(\sigma\)</span> è la deviazione standard del campione. Il massimo della funzione di log-verosimiglianza calcolata per via numerica, ovvero 30.93, è identico alla media dei dati campionari e corrisponde al risultato teorico atteso.</p>
</section>
</section>
<section id="commenti-e-considerazioni-finali">
<h2>Commenti e considerazioni finali<a class="headerlink" href="#commenti-e-considerazioni-finali" title="Permalink to this headline">#</a></h2>
<p>Nella funzione di verosimiglianza i dati (osservati) vengono trattati come fissi, mentre i valori del parametro (o dei parametri) <span class="math notranslate nohighlight">\(\theta\)</span> vengono variati: la verosimiglianza è una funzione di <span class="math notranslate nohighlight">\(\theta\)</span> per il dato fisso <span class="math notranslate nohighlight">\(y\)</span>. Pertanto, la funzione di verosimiglianza riassume i seguenti elementi: un modello statistico che genera stocasticamente i dati (in questo capitolo abbiamo esaminato due modelli statistici: quello binomiale e quello Normale), un intervallo di valori possibili per <span class="math notranslate nohighlight">\(\theta\)</span> e i dati osservati <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Nella statistica frequentista l’inferenza si basa solo sui dati a disposizione e qualunque informazione fornita dalle conoscenze precedenti non viene presa in considerazione. Nello specifico, nella statistica frequentista l’inferenza viene condotta massimizzando la funzione di (log) verosimiglianza, condizionatamente ai valori assunti dalle variabili casuali campionarie. Le basi dell’inferenza frequentista, dunque, sono state riassunte in questo Capitolo. Nella statistica bayesiana, invece, l’inferenza statistica viene condotta combinando la funzione di verosimiglianza con le distribuzioni a priori dei parametri incogniti <span class="math notranslate nohighlight">\(\theta\)</span>. Ciò verrà discusso nei Capitoli successivi.</p>
<p>La differenza fondamentale tra inferenza bayesiana e frequentista è dunque che i frequentisti non ritengono utile descrivere i parametri in termini probabilistici: i parametri dei modelli statistici vengono concepiti come fissi ma sconosciuti. Nell’inferenza bayesiana, invece, i parametri sconosciuti sono intesi come delle variabili casuali e ciò consente di quantificare in termini probabilistici il nostro grado di intertezza relativamente al loro valore.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="023_cont_rv_distr.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Distribuzioni di v.c. continue</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="025_intro_bayes.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Credibilità, modelli e parametri</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Corrado Caudek<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>